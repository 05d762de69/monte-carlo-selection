{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a9e526d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested cases: 15\n",
      "Cases ready     : 15\n",
      "OUT_DIR         : /home/hschatzle/monte-carlo-selection/results\n",
      "CERT_JSONL      : /home/hschatzle/monte-carlo-selection/results/certainty_metrics.jsonl\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 0. Batch case selection and paths (case-first layout)\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"..\").resolve()\n",
    "\n",
    "# Use the same cases you generated previously\n",
    "CASE_IDS = [f\"elephant_7_val_{i}\" for i in range(1, 16)]  # 11..26 inclusive\n",
    "\n",
    "ONNX_PATH = ROOT / \"data\" / \"models\" / \"resnet50_geirhos_tl_with_feats.onnx\"\n",
    "CLASS_NAMES_JSONL = ROOT / \"data\" / \"class_names.jsonl\"\n",
    "\n",
    "assert ONNX_PATH.exists(), f\"Missing ONNX model: {ONNX_PATH}\"\n",
    "assert CLASS_NAMES_JSONL.exists(), f\"Missing class_names.jsonl: {CLASS_NAMES_JSONL}\"\n",
    "\n",
    "# Output: write into results/certainty_metrics.jsonl (same as you used before)\n",
    "OUT_DIR = ROOT / \"results\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CERT_JSONL = OUT_DIR / \"certainty_metrics.jsonl\"\n",
    "\n",
    "# Helper to build per-case paths (matches previous notebook structure)\n",
    "def case_paths(case_id: str) -> dict:\n",
    "    case_dir = ROOT / \"data\" / \"cases\" / case_id\n",
    "    gen_dir  = case_dir / \"generated\"\n",
    "    paths = {\n",
    "        \"case_id\": case_id,\n",
    "        \"case_dir\": case_dir,\n",
    "        \"case_jsonl\": gen_dir / f\"{case_id}.jsonl\",\n",
    "        \"occluded_png\": case_dir / \"occluded.png\",\n",
    "        \"gt_png\": case_dir / \"gt.png\",\n",
    "        \"shapes_xy_npz\": gen_dir / \"shapes_xy.npz\",\n",
    "        \"completions_dir\": gen_dir / \"completions\",\n",
    "    }\n",
    "    return paths\n",
    "\n",
    "# Validate all cases exist and have expected files\n",
    "cases = []\n",
    "missing_any = False\n",
    "for cid in CASE_IDS:\n",
    "    p = case_paths(cid)\n",
    "    ok = True\n",
    "    if not p[\"case_dir\"].exists():\n",
    "        print(\"Missing CASE_DIR:\", p[\"case_dir\"])\n",
    "        ok = False\n",
    "    for k in [\"case_jsonl\", \"occluded_png\", \"gt_png\", \"shapes_xy_npz\", \"completions_dir\"]:\n",
    "        if not p[k].exists():\n",
    "            print(f\"Missing {k} for {cid}:\", p[k])\n",
    "            ok = False\n",
    "    if ok:\n",
    "        cases.append(p)\n",
    "    else:\n",
    "        missing_any = True\n",
    "\n",
    "print(\"Requested cases:\", len(CASE_IDS))\n",
    "print(\"Cases ready     :\", len(cases))\n",
    "print(\"OUT_DIR         :\", OUT_DIR)\n",
    "print(\"CERT_JSONL      :\", CERT_JSONL)\n",
    "if missing_any:\n",
    "    print(\"Some cases are missing required files. Only 'Cases ready' will be processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "904897ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded class list: 54\n",
      "First 10 classes: ['ant', 'bat', 'bear', 'bee', 'beetle', 'bird', 'bug', 'bull', 'butterfly', 'camel']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 1. Load class names (JSONL: one JSON object per line)\n",
    "import json\n",
    "\n",
    "with CLASS_NAMES_JSONL.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    classes = [json.loads(line)[\"class_name\"] for line in f if line.strip()]\n",
    "\n",
    "print(\"Loaded class list:\", len(classes))\n",
    "print(\"First 10 classes:\", classes[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84ed3fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 2. ONNX session + preprocessing (shared)\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "so = ort.SessionOptions()\n",
    "so.intra_op_num_threads = 8\n",
    "so.inter_op_num_threads = 1\n",
    "\n",
    "SESSION = ort.InferenceSession(\n",
    "    str(ONNX_PATH),\n",
    "    sess_options=so,\n",
    "    providers=[\"CPUExecutionProvider\"],\n",
    ")\n",
    "\n",
    "IN_NAME = SESSION.get_inputs()[0].name\n",
    "OUTS = [o.name for o in SESSION.get_outputs()]\n",
    "\n",
    "# These are the names you used in the other notebook\n",
    "LOGITS_OUT_NAME = \"output\"\n",
    "PENULT_OUT_NAME = \"features\"\n",
    "\n",
    "assert LOGITS_OUT_NAME in OUTS, f\"Model outputs {OUTS}, missing logits output '{LOGITS_OUT_NAME}'\"\n",
    "assert PENULT_OUT_NAME in OUTS, f\"Model outputs {OUTS}, missing penult output '{PENULT_OUT_NAME}'\"\n",
    "\n",
    "IM_SIZE = 224\n",
    "MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "def preprocess_png(p: str | Path) -> np.ndarray:\n",
    "    img = Image.open(p).convert(\"RGB\").resize((IM_SIZE, IM_SIZE), resample=Image.BILINEAR)\n",
    "    x = np.asarray(img, dtype=np.float32) / 255.0\n",
    "    x = (x - MEAN[None, None, :]) / STD[None, None, :]\n",
    "    x = np.transpose(x, (2, 0, 1))[None, ...]  # (1,3,224,224)\n",
    "    return x.astype(np.float32, copy=False)\n",
    "\n",
    "def infer_penult_and_logits(p: str | Path):\n",
    "    x = preprocess_png(p)\n",
    "    pen, logits = SESSION.run([PENULT_OUT_NAME, LOGITS_OUT_NAME], {IN_NAME: x})\n",
    "    return np.asarray(pen), np.asarray(logits).reshape(-1)\n",
    "\n",
    "def softmax(logits: np.ndarray) -> np.ndarray:\n",
    "    z = np.asarray(logits, dtype=np.float64).reshape(-1)\n",
    "    z = z - np.max(z)\n",
    "    e = np.exp(z)\n",
    "    return e / (np.sum(e) + 1e-12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44a9faa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example case: elephant_7_val_1\n",
      "N completions: 10000\n",
      "Missing PNGs : 0\n",
      "polygons_xy  : (10000,) object\n",
      "matlab_1_idx : True\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 3. Load shapes_xy.npz for a case and resolve completion PNG paths\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def load_case_shapes(case: dict):\n",
    "    z = np.load(case[\"shapes_xy_npz\"], allow_pickle=True)\n",
    "    out_files_raw = z[\"out_files\"].tolist()\n",
    "    polygons_xy   = z[\"polygons\"]\n",
    "    matlab_1_indexed = bool(z[\"matlab_1_indexed\"]) if \"matlab_1_indexed\" in z else False\n",
    "\n",
    "    completions_dir = case[\"completions_dir\"]\n",
    "\n",
    "    def resolve_png_path(p: str) -> Path:\n",
    "        pth = Path(p)\n",
    "        if pth.exists():\n",
    "            return pth\n",
    "        return (completions_dir / pth.name)\n",
    "\n",
    "    png_paths = [resolve_png_path(p) for p in out_files_raw]\n",
    "    missing = sum(1 for p in png_paths if not p.exists())\n",
    "\n",
    "    return png_paths, polygons_xy, matlab_1_indexed, missing\n",
    "\n",
    "# quick check first ready case\n",
    "if len(cases) > 0:\n",
    "    png_paths0, polys0, m10, miss0 = load_case_shapes(cases[0])\n",
    "    print(\"Example case:\", cases[0][\"case_id\"])\n",
    "    print(\"N completions:\", len(png_paths0))\n",
    "    print(\"Missing PNGs :\", miss0)\n",
    "    print(\"polygons_xy  :\", polys0.shape, polys0.dtype)\n",
    "    print(\"matlab_1_idx :\", m10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9242cfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example case: elephant_7_val_1\n",
      "TARGET idx: 21 class: elephant\n",
      "baseline logit: 7.087069988250732 prob: 0.6987596641956342\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 4. Compute target class per case from occluded image\n",
    "# Target = argmax logits on occluded baseline (same as your current logic)\n",
    "\n",
    "def target_from_occluded(case: dict):\n",
    "    _, occ_logits = infer_penult_and_logits(case[\"occluded_png\"])\n",
    "    occ_prob = softmax(occ_logits)\n",
    "    target_idx = int(np.argmax(occ_logits))\n",
    "    return target_idx, float(occ_logits[target_idx]), float(occ_prob[target_idx])\n",
    "\n",
    "# sanity\n",
    "if len(cases) > 0:\n",
    "    tidx, tlog0, tpr0 = target_from_occluded(cases[0])\n",
    "    print(\"Example case:\", cases[0][\"case_id\"])\n",
    "    print(\"TARGET idx:\", tidx, \"class:\", classes[tidx])\n",
    "    print(\"baseline logit:\", tlog0, \"prob:\", tpr0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d966d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell. Robust parallel scoring (always returns tlog,tpr,tmar,tdel)\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "N_WORKERS = 8\n",
    "\n",
    "def _score_chunk_onnx(args):\n",
    "    \"\"\"\n",
    "    args = (chunk_pairs, target_idx, occ_tlog, onnx_path_str, logits_out_name)\n",
    "    chunk_pairs: list of (i, path_str)\n",
    "    Returns: (idxs, tlog_vals, tpr_vals, tmar_vals, tdel_vals, n_scored)\n",
    "    \"\"\"\n",
    "    chunk_pairs, target_idx, occ_tlog, onnx_path_str, logits_out_name = args\n",
    "\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import onnxruntime as ort\n",
    "    from PIL import Image\n",
    "\n",
    "    so = ort.SessionOptions()\n",
    "    so.intra_op_num_threads = 1\n",
    "    so.inter_op_num_threads = 1\n",
    "\n",
    "    sess = ort.InferenceSession(onnx_path_str, sess_options=so, providers=[\"CPUExecutionProvider\"])\n",
    "    in_name = sess.get_inputs()[0].name\n",
    "\n",
    "    IM_SIZE = 224\n",
    "    MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "    def preprocess(p):\n",
    "        img = Image.open(p).convert(\"RGB\").resize((IM_SIZE, IM_SIZE), resample=Image.BILINEAR)\n",
    "        x = np.asarray(img, dtype=np.float32) / 255.0\n",
    "        x = (x - MEAN[None, None, :]) / STD[None, None, :]\n",
    "        x = np.transpose(x, (2, 0, 1))[None, ...]\n",
    "        return x.astype(np.float32, copy=False)\n",
    "\n",
    "    def softmax_local(logits):\n",
    "        z = np.asarray(logits, dtype=np.float64).reshape(-1)\n",
    "        z = z - np.max(z)\n",
    "        e = np.exp(z)\n",
    "        return e / (np.sum(e) + 1e-12)\n",
    "\n",
    "    idxs = []\n",
    "    v_tlog = []\n",
    "    v_tpr  = []\n",
    "    v_tmar = []\n",
    "    v_tdel = []\n",
    "\n",
    "    for i, pstr in chunk_pairs:\n",
    "        try:\n",
    "            if not os.path.exists(pstr):\n",
    "                continue\n",
    "\n",
    "            x = preprocess(pstr)\n",
    "            logits = sess.run([logits_out_name], {in_name: x})[0]\n",
    "            logits = np.asarray(logits).reshape(-1)\n",
    "\n",
    "            prob = softmax_local(logits)\n",
    "            tl = float(logits[int(target_idx)])\n",
    "            other_max = float(np.max(np.delete(logits, int(target_idx))))\n",
    "\n",
    "            idxs.append(int(i))\n",
    "            v_tlog.append(tl)\n",
    "            v_tpr.append(float(prob[int(target_idx)]))\n",
    "            v_tdel.append(float(tl - float(occ_tlog)))\n",
    "            v_tmar.append(float(tl - other_max))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return (\n",
    "        np.asarray(idxs, dtype=np.int64),\n",
    "        np.asarray(v_tlog, dtype=np.float64),\n",
    "        np.asarray(v_tpr, dtype=np.float64),\n",
    "        np.asarray(v_tmar, dtype=np.float64),\n",
    "        np.asarray(v_tdel, dtype=np.float64),\n",
    "        int(len(idxs)),\n",
    "    )\n",
    "\n",
    "\n",
    "def score_case_parallel(case: dict, png_paths: list, target_idx: int, occ_tlog: float):\n",
    "    N = len(png_paths)\n",
    "    tlog = np.full(N, np.nan, dtype=np.float64)\n",
    "    tpr  = np.full(N, np.nan, dtype=np.float64)\n",
    "    tmar = np.full(N, np.nan, dtype=np.float64)\n",
    "    tdel = np.full(N, np.nan, dtype=np.float64)\n",
    "\n",
    "    pairs = [(i, str(p)) for i, p in enumerate(png_paths)]\n",
    "    if len(pairs) == 0:\n",
    "        print(\"No png_paths for case:\", case.get(\"case_id\", \"unknown\"))\n",
    "        return tlog, tpr, tmar, tdel\n",
    "\n",
    "    chunk_size = max(1, len(pairs) // (N_WORKERS * 8))\n",
    "    chunks = [pairs[j:j+chunk_size] for j in range(0, len(pairs), chunk_size)]\n",
    "\n",
    "    args_list = [(ch, target_idx, occ_tlog, str(ONNX_PATH), LOGITS_OUT_NAME) for ch in chunks]\n",
    "\n",
    "    n_scored_total = 0\n",
    "    with ProcessPoolExecutor(max_workers=N_WORKERS) as ex:\n",
    "        futs = [ex.submit(_score_chunk_onnx, a) for a in args_list]\n",
    "        for fut in tqdm(as_completed(futs), total=len(futs), desc=f\"Scoring {case.get('case_id','case')}\"):\n",
    "            idxs, a, b, c, d, n_scored = fut.result()\n",
    "            if idxs.size:\n",
    "                tlog[idxs] = a\n",
    "                tpr[idxs]  = b\n",
    "                tmar[idxs] = c\n",
    "                tdel[idxs] = d\n",
    "            n_scored_total += int(n_scored)\n",
    "\n",
    "    finite = int(np.isfinite(tlog).sum())\n",
    "    if finite == 0:\n",
    "        print(\"WARNING: scored 0 images for\", case.get(\"case_id\", \"case\"))\n",
    "        print(\"  N paths:\", N)\n",
    "        print(\"  Example path:\", str(png_paths[0]) if N else \"NA\")\n",
    "        print(\"  ONNX_PATH:\", str(ONNX_PATH))\n",
    "        print(\"  LOGITS_OUT_NAME:\", LOGITS_OUT_NAME)\n",
    "\n",
    "    return tlog, tpr, tmar, tdel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a87238ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will append rows to: /home/hschatzle/monte-carlo-selection/results/certainty_metrics.jsonl\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 6. Choose best completion (by target margin or target logit) and write JSONL row\n",
    "# Uses the same output schema you showed earlier.\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def certainty_metrics_from_logits(logits: np.ndarray) -> dict:\n",
    "    z = np.asarray(logits, dtype=np.float64).reshape(-1)\n",
    "    p = softmax(z)\n",
    "    k = int(np.argmax(z))\n",
    "    z_top = float(z[k])\n",
    "    z_2nd = float(np.max(np.delete(z, k))) if z.size > 1 else float(\"-inf\")\n",
    "    margin = z_top - z_2nd\n",
    "    p_top = float(np.max(p))\n",
    "    entropy = float(-np.sum(p * np.log(p + 1e-12)))\n",
    "    # logsumexp\n",
    "    zmax = float(np.max(z))\n",
    "    energy = float(np.log(np.sum(np.exp(z - zmax)) + 1e-12) + zmax)\n",
    "    return {\n",
    "        \"pred_idx\": k,\n",
    "        \"pred_logit\": z_top,\n",
    "        \"margin_top2\": float(margin),\n",
    "        \"p_top1\": float(p_top),\n",
    "        \"entropy\": float(entropy),\n",
    "        \"energy_logsumexp\": float(energy),\n",
    "    }\n",
    "\n",
    "def cls_name(idx: int) -> str:\n",
    "    return classes[int(idx)]\n",
    "\n",
    "def pick_best_index(tlog: np.ndarray, tmar: np.ndarray, *, prefer: str = \"margin\") -> int:\n",
    "    finite = np.isfinite(tlog) & np.isfinite(tmar)\n",
    "    if int(finite.sum()) == 0:\n",
    "        raise RuntimeError(\"No finite scores to pick best completion.\")\n",
    "    idxs = np.where(finite)[0]\n",
    "    if prefer == \"margin\":\n",
    "        j = idxs[np.argmax(tmar[idxs])]\n",
    "    elif prefer == \"logit\":\n",
    "        j = idxs[np.argmax(tlog[idxs])]\n",
    "    else:\n",
    "        raise ValueError(\"prefer must be 'margin' or 'logit'\")\n",
    "    return int(j)\n",
    "\n",
    "def append_certainty_row(case: dict, best_png: Path):\n",
    "    # infer logits for best, gt, occ\n",
    "    _, logits_best = infer_penult_and_logits(best_png)\n",
    "    _, logits_gt   = infer_penult_and_logits(case[\"gt_png\"])\n",
    "    _, logits_occ  = infer_penult_and_logits(case[\"occluded_png\"])\n",
    "\n",
    "    m_best = certainty_metrics_from_logits(logits_best)\n",
    "    m_gt   = certainty_metrics_from_logits(logits_gt)\n",
    "    m_occ  = certainty_metrics_from_logits(logits_occ)\n",
    "\n",
    "    metrics_def = {\n",
    "        \"margin_top2\": {\"direction\": \"higher = more certain\"},\n",
    "        \"p_top1\": {\"direction\": \"higher = more certain\"},\n",
    "        \"entropy\": {\"direction\": \"lower = more certain\"},\n",
    "        \"energy_logsumexp\": {\"direction\": \"higher = more certain\"},\n",
    "    }\n",
    "\n",
    "    row = {\n",
    "        \"timestamp_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        \"case_id\": case[\"case_id\"],\n",
    "        \"best_png\": str(best_png),\n",
    "        \"gt_png\": str(case[\"gt_png\"]),\n",
    "        \"occ_png\": str(case[\"occluded_png\"]),\n",
    "        \"best_pred_idx\": int(m_best[\"pred_idx\"]),\n",
    "        \"gt_pred_idx\": int(m_gt[\"pred_idx\"]),\n",
    "        \"occ_pred_idx\": int(m_occ[\"pred_idx\"]),\n",
    "        \"best_pred_class\": cls_name(m_best[\"pred_idx\"]),\n",
    "        \"gt_pred_class\": cls_name(m_gt[\"pred_idx\"]),\n",
    "        \"occ_pred_class\": cls_name(m_occ[\"pred_idx\"]),\n",
    "        \"metrics\": {\n",
    "            k: {\n",
    "                \"best\": float(m_best[k]),\n",
    "                \"gt\": float(m_gt[k]),\n",
    "                \"occ\": float(m_occ[k]),\n",
    "                \"direction\": metrics_def[k][\"direction\"],\n",
    "            }\n",
    "            for k in metrics_def.keys()\n",
    "        },\n",
    "        \"deltas\": {\n",
    "            \"best_minus_gt\": {\n",
    "                k: float(m_best[k] - m_gt[k]) for k in metrics_def.keys()\n",
    "            },\n",
    "            \"gt_minus_occ\": {\n",
    "                k: float(m_gt[k] - m_occ[k]) for k in metrics_def.keys()\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    with CERT_JSONL.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    return row\n",
    "\n",
    "print(\"Will append rows to:\", CERT_JSONL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ab1bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c3366a19434e93842da40032dc52a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring elephant_7_val_1:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CASE: elephant_7_val_1\n",
      "  target: elephant idx: 21 occ_logit: 7.087069988250732\n",
      "  best: completion_0001_06673.png\n",
      "  best target_logit: 7.2816033363342285 best target_margin: 1.891324520111084 best target_prob: 0.6668654229684642\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7666ac405ea4899a02ea1046380d755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring elephant_7_val_2:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CASE: elephant_7_val_2\n",
      "  target: elephant idx: 21 occ_logit: 7.087069988250732\n",
      "  best: completion_0001_03881.png\n",
      "  best target_logit: 7.3613600730896 best target_margin: 1.9310007095336914 best target_prob: 0.6694445294113563\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b898bcbd214b6e92ff98f0931c7546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring elephant_7_val_3:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CASE: elephant_7_val_3\n",
      "  target: elephant idx: 21 occ_logit: 7.087069988250732\n",
      "  best: completion_0001_01436.png\n",
      "  best target_logit: 7.332314491271973 best target_margin: 1.9637165069580078 best target_prob: 0.6637961821264552\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbea221dd324a16a48c3ecdde48ab80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring elephant_7_val_4:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CASE: elephant_7_val_4\n",
      "  target: elephant idx: 21 occ_logit: 7.087069988250732\n",
      "  best: completion_0001_05537.png\n",
      "  best target_logit: 7.148738861083984 best target_margin: 1.8863897323608398 best target_prob: 0.6392780543373996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f418061d61be41d0a9b4fdb94d9a1ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring elephant_7_val_5:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 7. Run the batch: for each case, score completions, pick best, append row\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "PICK_RULE = \"margin\"  # \"margin\" or \"logit\"\n",
    "\n",
    "rows_written = []\n",
    "for case in cases:\n",
    "    png_paths, polygons_xy, matlab_1_indexed, missing = load_case_shapes(case)\n",
    "    if missing > 0:\n",
    "        print(\"Warning:\", case[\"case_id\"], \"missing PNGs:\", missing)\n",
    "\n",
    "    target_idx, occ_tlog, occ_tpr = target_from_occluded(case)\n",
    "\n",
    "    # Score all completions in parallel (logits for target, margins, etc.)\n",
    "    tlog, tpr, tmar, tdel = score_case_parallel(case, png_paths, target_idx, occ_tlog)\n",
    "\n",
    "    best_i = pick_best_index(tlog, tmar, prefer=PICK_RULE)\n",
    "    best_png = png_paths[best_i]\n",
    "\n",
    "    print(\"\\nCASE:\", case[\"case_id\"])\n",
    "    print(\"  target:\", classes[target_idx], \"idx:\", target_idx, \"occ_logit:\", occ_tlog)\n",
    "    print(\"  best:\", best_png.name)\n",
    "    print(\"  best target_logit:\", float(tlog[best_i]), \"best target_margin:\", float(tmar[best_i]), \"best target_prob:\", float(tpr[best_i]))\n",
    "\n",
    "    row = append_certainty_row(case, best_png)\n",
    "    rows_written.append(row)\n",
    "\n",
    "print(\"\\nDone. Rows appended:\", len(rows_written))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (analysis)",
   "language": "python",
   "name": "analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
