{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49904907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx import helper, numpy_helper, shape_inference\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b474a50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits source tensor: output\n",
      "Feats source tensor: /Flatten_output_0\n"
     ]
    }
   ],
   "source": [
    "ONNX_IN  = Path(\"/home/hschatzle/monte-carlo-selection/data/models/resnet50_tl_20250829.onnx\")\n",
    "ONNX_OUT = Path(\"/home/hschatzle/monte-carlo-selection/data/models/resnet50_tl_20250829_with_feats.onnx\")\n",
    "\n",
    "\n",
    "model = onnx.load(str(ONNX_IN))\n",
    "\n",
    "# logits tensor is the current graph output name (your model: \"output\")\n",
    "assert len(model.graph.output) == 1, \"Expected single output for logits\"\n",
    "logits_src = model.graph.output[0].name\n",
    "print(\"Logits source tensor:\", logits_src)\n",
    "\n",
    "# penultimate features tensor: last Flatten output (right before Gemm)\n",
    "flatten_nodes = [n for n in model.graph.node if n.op_type == \"Flatten\"]\n",
    "assert flatten_nodes, \"No Flatten node found\"\n",
    "feats_src = flatten_nodes[-1].output[0]\n",
    "print(\"Feats source tensor:\", feats_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b326bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /home/hschatzle/monte-carlo-selection/data/models/resnet50_tl_20250829_with_feats.onnx\n"
     ]
    }
   ],
   "source": [
    "# Cell 2. Add Identity heads for both outputs (stable naming) + set graph outputs (feats + logits)\n",
    "import onnx\n",
    "from onnx import helper, TensorProto, shape_inference\n",
    "\n",
    "# Infer shapes if possible (so outputs have known shapes)\n",
    "model_inf = shape_inference.infer_shapes(model)\n",
    "\n",
    "def find_vi(name: str):\n",
    "    # Search existing value_info / inputs / outputs\n",
    "    for vi in list(model_inf.graph.value_info) + list(model_inf.graph.input) + list(model_inf.graph.output):\n",
    "        if vi.name == name:\n",
    "            return vi\n",
    "    return None\n",
    "\n",
    "def make_output_vi(src_name: str, out_name: str):\n",
    "    vi = find_vi(src_name)\n",
    "    if vi is None:\n",
    "        # Fallback: unknown shape\n",
    "        return helper.make_tensor_value_info(out_name, TensorProto.FLOAT, None)\n",
    "\n",
    "    # Copy type/shape from src, but rename to out_name\n",
    "    t = vi.type.tensor_type\n",
    "    elem_type = t.elem_type\n",
    "    shape = [d.dim_value if d.dim_value > 0 else (d.dim_param if d.dim_param else None) for d in t.shape.dim]\n",
    "    # ONNX wants dim_value or dim_param, cannot mix None directly. If unknown, omit the whole shape.\n",
    "    if any(s is None for s in shape):\n",
    "        return helper.make_tensor_value_info(out_name, elem_type, None)\n",
    "    return helper.make_tensor_value_info(out_name, elem_type, shape)\n",
    "\n",
    "# Create explicit output names\n",
    "FEATS_OUT  = \"features\"\n",
    "LOGITS_OUT = \"logits\"\n",
    "\n",
    "# Add Identity nodes to expose them as named outputs\n",
    "id_feats  = helper.make_node(\"Identity\", inputs=[feats_src],  outputs=[FEATS_OUT],  name=\"ExposeFeats\")\n",
    "id_logits = helper.make_node(\"Identity\", inputs=[logits_src], outputs=[LOGITS_OUT], name=\"ExposeLogits\")\n",
    "\n",
    "model.graph.node.extend([id_feats, id_logits])\n",
    "\n",
    "# Replace graph outputs with both\n",
    "model.graph.ClearField(\"output\")\n",
    "model.graph.output.extend([\n",
    "    make_output_vi(feats_src, FEATS_OUT),\n",
    "    make_output_vi(logits_src, LOGITS_OUT),\n",
    "])\n",
    "\n",
    "onnx.save(model, str(ONNX_OUT))\n",
    "print(\"Saved:\", ONNX_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e0cb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      "  features ['batch_size', 2048] tensor(float)\n",
      "  logits ['batch_size', 54] tensor(float)\n",
      "\n",
      "Inputs:\n",
      "  input ['batch_size', 3, 224, 224] tensor(float)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3. Verify with ONNX Runtime\n",
    "import onnxruntime as ort\n",
    "\n",
    "sess = ort.InferenceSession(str(ONNX_OUT), providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "print(\"Outputs:\")\n",
    "for o in sess.get_outputs():\n",
    "    print(\" \", o.name, o.shape, o.type)\n",
    "\n",
    "print(\"\\nInputs:\")\n",
    "for i in sess.get_inputs():\n",
    "    print(\" \", i.name, i.shape, i.type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
