{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44ee3876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cases:\n",
      "  dog_8_val_11\n",
      "  dog_8_val_12\n",
      "  dog_8_val_13\n",
      "  dog_8_val_14\n",
      "  dog_8_val_15\n",
      "  dog_8_val_16\n",
      "  dog_8_val_17\n",
      "  dog_8_val_18\n",
      "  dog_8_val_19\n",
      "  dog_8_val_20\n",
      "  dog_8_val_21\n",
      "  dog_8_val_22\n",
      "  dog_8_val_23\n",
      "  dog_8_val_24\n",
      "  dog_8_val_25\n",
      "  dog_8_val_26\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "ROOT = Path(\"..\").resolve()\n",
    "\n",
    "BASE_CASE = \"dog_8_val_5\"\n",
    "BASE_JSONL = ROOT / \"data\" / \"cases\" / BASE_CASE / \"generated\" / f\"{BASE_CASE}.jsonl\"\n",
    "\n",
    "CASE_SUFFIXES = list(range(11, 27))   # 11..26 inclusive\n",
    "\n",
    "MASTER_LIB = ROOT / \"data\" / \"master\" / \"master_lib.mat\"\n",
    "\n",
    "H, W = 600, 900\n",
    "MARGIN = 5\n",
    "SUPERSAMPLE = 4\n",
    "\n",
    "def ensure_case_jsonl(case_name: str) -> Path:\n",
    "    out_dir = ROOT / \"data\" / \"cases\" / case_name\n",
    "    gen_dir = out_dir / \"generated\"\n",
    "    gen_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    target = gen_dir / f\"{case_name}.jsonl\"\n",
    "    if not target.exists():\n",
    "        shutil.copy2(BASE_JSONL, target)\n",
    "    return target\n",
    "\n",
    "print(\"Creating cases:\")\n",
    "for s in CASE_SUFFIXES:\n",
    "    print(\" \", f\"dog_8_val_{s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d31791c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded records: 54000\n",
      "Classes: 54\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(str(ROOT / \"scripts\"))\n",
    "\n",
    "from shape_gen.io_mat import load_generated_case, unit_to_pixel\n",
    "from shape_gen.library import load_master_records, build_class_index\n",
    "\n",
    "records = load_master_records(MASTER_LIB)\n",
    "classes, byClass = build_class_index(records)\n",
    "\n",
    "print(\"Loaded records:\", len(records))\n",
    "print(\"Classes:\", len(classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02cb0854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_case_from_jsonl(jsonl_path: Path, default_baseGrid: int = 256):\n",
    "    row = json.loads(jsonl_path.read_text().splitlines()[-1])\n",
    "\n",
    "    if \"silhouette_u\" in row:\n",
    "        sil_u = np.asarray(row[\"silhouette_u\"], float)\n",
    "        occ_u = np.asarray(row.get(\"occluder_u\", []), float)\n",
    "        baseGrid = int(row[\"baseGrid\"])\n",
    "        sil_class = row.get(\"sil_class\", None)\n",
    "\n",
    "    else:\n",
    "        sil_u = np.asarray(row[\"shape_contour_xy\"], float)\n",
    "        occ_u = np.asarray(row.get(\"occluder_rect_xy\", []), float)\n",
    "        baseGrid = int(row.get(\"baseGrid\", default_baseGrid))\n",
    "        sil_class = row.get(\"sil_class\", row.get(\"category\", None))\n",
    "\n",
    "    if occ_u.size == 0:\n",
    "        occ_u = np.zeros((0,2), float)\n",
    "\n",
    "    return sil_u, occ_u, baseGrid, sil_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb04e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Cell 4. Worker + merge utilities (FIXED save_xy_npz signature)\n",
    "from pathlib import Path\n",
    "import re\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "from shape_gen.generate_parallel import generate_completions, save_metadata_jsonl\n",
    "from shape_gen.heatmap.xy_store import save_xy_npz\n",
    "\n",
    "\n",
    "def _worker_run(args):\n",
    "    wid = int(args[\"worker_id\"])\n",
    "    out_dir = Path(args[\"out_dir\"])\n",
    "    tmp_dir = Path(args[\"tmp_dir\"])\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rng = np.random.default_rng(int(args[\"seed\"]))\n",
    "\n",
    "    metas, out_files_xy, polygons_xy = generate_completions(\n",
    "        silhouette=args[\"silhouette\"],\n",
    "        occluder=args[\"occluder\"],\n",
    "        start_pt=args[\"start_pt\"],\n",
    "        end_pt=args[\"end_pt\"],\n",
    "        minX=int(args[\"minX\"]),\n",
    "        minY=int(args[\"minY\"]),\n",
    "        wBB=int(args[\"wBB\"]),\n",
    "        hBB=int(args[\"hBB\"]),\n",
    "        out_w=int(args[\"out_w\"]),\n",
    "        out_h=int(args[\"out_h\"]),\n",
    "        out_dir=out_dir,\n",
    "        silhouette_index=1,\n",
    "        sil_class=args[\"sil_class\"],\n",
    "        base_grid=int(args[\"base_grid\"]),\n",
    "        records=args[\"records\"],\n",
    "        classes=args[\"classes\"],\n",
    "        byClass=args[\"byClass\"],\n",
    "        n_images=int(args[\"n_attempts\"]),\n",
    "        rng=rng,\n",
    "        start_index=int(args[\"start_index\"]),\n",
    "        fraction=float(args[\"fraction\"]),\n",
    "        final_n_samples_mode=args[\"final_n_samples_mode\"],\n",
    "        supersample=int(args[\"supersample\"]),\n",
    "        flush_every=int(args[\"flush_every\"]),\n",
    "        max_attempts_per_image=int(args[\"max_attempts_per_image\"]),\n",
    "        require_valid=True,\n",
    "        snap_intersections_to_vertices=True,\n",
    "        refit_enabled=False,\n",
    "        refit_n_ctrl=int(args[\"refit_n_ctrl\"]),\n",
    "        refit_subdiv=int(args[\"refit_subdiv\"]),\n",
    "        refit_jitter_sigma=float(args[\"refit_jitter_sigma\"]),\n",
    "        refit_max_attempts=int(args[\"refit_max_attempts\"]),\n",
    "        shrink_gamma=float(args[\"shrink_gamma\"]),\n",
    "        max_shrink_iters=int(args[\"max_shrink_iters\"]),\n",
    "        smooth_win=int(args[\"smooth_win\"]),\n",
    "        try_mirror=True,\n",
    "        save_invalid=False,\n",
    "        invalid_subdir=\"_invalid\",\n",
    "    )\n",
    "\n",
    "    meta_path = tmp_dir / f\"meta_{wid:02d}.jsonl\"\n",
    "    save_metadata_jsonl(metas, meta_path)\n",
    "\n",
    "    xy_path = tmp_dir / f\"xy_{wid:02d}.npz\"\n",
    "    save_xy_npz(\n",
    "        xy_path,\n",
    "        out_files=out_files_xy,\n",
    "        polygons=polygons_xy,\n",
    "        base_grid=int(args[\"base_grid\"]),\n",
    "        matlab_1_indexed=True,\n",
    "    )\n",
    "\n",
    "    return str(meta_path), str(xy_path)\n",
    "\n",
    "\n",
    "def extract_idx(p: str) -> int:\n",
    "    m = re.search(r\"completion_\\d{4}_(\\d{5})\\.png$\", str(p))\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not parse completion index from: {p}\")\n",
    "    return int(m.group(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CASE: dog_8_val_11\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "from shape_gen.geom_bbox import compute_bbox\n",
    "from shape_gen.render import draw_and_save\n",
    "from shape_gen.intersections2 import find_intersection_points_multiple\n",
    "\n",
    "N_WORKERS = 8\n",
    "TOTAL_ATTEMPTS = 10_000\n",
    "ATTEMPTS_PER = TOTAL_ATTEMPTS // N_WORKERS\n",
    "assert ATTEMPTS_PER * N_WORKERS == TOTAL_ATTEMPTS\n",
    "\n",
    "FRACTION = 0.22\n",
    "SAMPLES_MODE = \"matlab_100\"\n",
    "MAX_ATTEMPTS = 250\n",
    "\n",
    "SHRINK_GAMMA = 0.82\n",
    "MAX_SHRINK_ITERS = 60\n",
    "SMOOTH_WIN = 7\n",
    "\n",
    "REFIT_N_CTRL = 10\n",
    "REFIT_SUBDIV = 18\n",
    "REFIT_JITTER = 0.008\n",
    "REFIT_MAX_ATTEMPTS = 12\n",
    "\n",
    "FLUSH = 1000\n",
    "\n",
    "for suffix in CASE_SUFFIXES:\n",
    "\n",
    "    CASE = f\"dog_8_val_{suffix}\"\n",
    "    print(\"\\n=== CASE:\", CASE)\n",
    "\n",
    "    jsonl_path = ensure_case_jsonl(CASE)\n",
    "\n",
    "    OUT_DIR = ROOT / \"data\" / \"cases\" / CASE\n",
    "    RAND_DIR = OUT_DIR / \"generated\" / \"completions\"\n",
    "    TMP = OUT_DIR / \"generated\" / \"_tmp\"\n",
    "\n",
    "    RAND_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    TMP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    sil_u, occ_u, baseGrid, sil_class = load_case_from_jsonl(jsonl_path)\n",
    "\n",
    "    silhouette = unit_to_pixel(sil_u, baseGrid)\n",
    "    occluder = unit_to_pixel(occ_u, baseGrid) if occ_u.size else np.zeros((0, 2), dtype=np.int32)\n",
    "\n",
    "    polys = [silhouette] + ([occluder] if occluder.size else [])\n",
    "    minX, minY, wBB, hBB = compute_bbox(\n",
    "        polys,\n",
    "        base_grid=baseGrid,\n",
    "        margin=MARGIN\n",
    "    )\n",
    "\n",
    "    # Write GT\n",
    "    draw_and_save(\n",
    "        polygons=[silhouette],\n",
    "        colors=[[0, 0, 0]],\n",
    "        minX=minX, minY=minY, wBB=wBB, hBB=hBB,\n",
    "        out_w=W, out_h=H,\n",
    "        out_file=OUT_DIR / \"gt.png\",\n",
    "        supersample=SUPERSAMPLE\n",
    "    )\n",
    "\n",
    "    # Write occluded (only if occluder exists)\n",
    "    if occluder.size:\n",
    "        draw_and_save(\n",
    "            polygons=[silhouette, occluder],\n",
    "            colors=[[0, 0, 0], [131, 131, 131]],\n",
    "            minX=minX, minY=minY, wBB=wBB, hBB=hBB,\n",
    "            out_w=W, out_h=H,\n",
    "            out_file=OUT_DIR / \"occluded.png\",\n",
    "            supersample=SUPERSAMPLE\n",
    "        )\n",
    "    else:\n",
    "        print(\"Occluder empty. Skipping occluded render.\")\n",
    "\n",
    "    # Intersections required for completion generation\n",
    "    if not occluder.size:\n",
    "        raise RuntimeError(f\"{CASE}: occluder is empty. Cannot generate completions without intersections.\")\n",
    "\n",
    "    pts = find_intersection_points_multiple(silhouette, occluder, eps_merge=1e-3)\n",
    "    if pts.shape[0] < 2:\n",
    "        raise RuntimeError(f\"{CASE}: <2 intersection points found. Got {pts.shape[0]}\")\n",
    "\n",
    "    start_pt, end_pt = pts[0], pts[1]\n",
    "\n",
    "    CASE_SEED = int(suffix)\n",
    "\n",
    "    jobs = []\n",
    "    for wid in range(N_WORKERS):\n",
    "\n",
    "        worker_seed = CASE_SEED * 1_000_000 + wid * 10_000 + 123\n",
    "\n",
    "        jobs.append(dict(\n",
    "            worker_id=wid,\n",
    "            seed=worker_seed,\n",
    "            tmp_dir=str(TMP),\n",
    "            out_dir=str(RAND_DIR),\n",
    "\n",
    "            silhouette=silhouette,\n",
    "            occluder=occluder,\n",
    "            start_pt=start_pt,\n",
    "            end_pt=end_pt,\n",
    "            minX=minX,\n",
    "            minY=minY,\n",
    "            wBB=wBB,\n",
    "            hBB=hBB,\n",
    "            out_w=W,\n",
    "            out_h=H,\n",
    "            sil_class=sil_class,\n",
    "            base_grid=baseGrid,\n",
    "\n",
    "            records=records,\n",
    "            classes=classes,\n",
    "            byClass=byClass,\n",
    "\n",
    "            n_attempts=ATTEMPTS_PER,\n",
    "            start_index=wid * ATTEMPTS_PER + 1,\n",
    "\n",
    "            fraction=FRACTION,\n",
    "            final_n_samples_mode=SAMPLES_MODE,\n",
    "            supersample=SUPERSAMPLE,\n",
    "            flush_every=FLUSH,\n",
    "            max_attempts_per_image=MAX_ATTEMPTS,\n",
    "\n",
    "            refit_n_ctrl=REFIT_N_CTRL,\n",
    "            refit_subdiv=REFIT_SUBDIV,\n",
    "            refit_jitter_sigma=REFIT_JITTER,\n",
    "            refit_max_attempts=REFIT_MAX_ATTEMPTS,\n",
    "\n",
    "            shrink_gamma=SHRINK_GAMMA,\n",
    "            max_shrink_iters=MAX_SHRINK_ITERS,\n",
    "            smooth_win=SMOOTH_WIN,\n",
    "        ))\n",
    "\n",
    "    results = []\n",
    "    with ProcessPoolExecutor(max_workers=N_WORKERS) as ex:\n",
    "        for fut in as_completed([ex.submit(_worker_run, j) for j in jobs]):\n",
    "            results.append(fut.result())\n",
    "\n",
    "    # ---- merge meta\n",
    "    all_meta = []\n",
    "    for m, _ in results:\n",
    "        all_meta += Path(m).read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "    meta_rows = [json.loads(l) for l in all_meta if l.strip()]\n",
    "    meta_rows.sort(key=lambda r: int(r.get(\"completion_index\", 0)))\n",
    "\n",
    "    meta_out = OUT_DIR / \"generated\" / \"shapes_meta.jsonl\"\n",
    "    meta_out.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    with meta_out.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in meta_rows:\n",
    "            f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "    # ---- merge xy\n",
    "    pairs = []\n",
    "    for _, xy in results:\n",
    "        npz = np.load(xy, allow_pickle=True)\n",
    "        for fpath, poly in zip(npz[\"out_files\"], npz[\"polygons\"]):\n",
    "            pairs.append((extract_idx(fpath), str(fpath), poly))\n",
    "\n",
    "    pairs.sort(key=lambda t: t[0])\n",
    "\n",
    "    out_files = [p[1] for p in pairs]\n",
    "    polys_xy = [p[2] for p in pairs]\n",
    "\n",
    "    xy_out = OUT_DIR / \"generated\" / \"shapes_xy.npz\"\n",
    "    save_xy_npz(\n",
    "        xy_out,\n",
    "        out_files=out_files,\n",
    "        polygons=polys_xy,\n",
    "        base_grid=int(baseGrid),\n",
    "        matlab_1_indexed=True,\n",
    "    )\n",
    "\n",
    "    print(\"Saved:\", CASE, \" valid:\", len(out_files))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (analysis)",
   "language": "python",
   "name": "analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
